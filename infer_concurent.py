import os
import sys
import numpy as np
import tensorflow as tf
import torch
import clip
import cv2
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import argparse
from PIL import Image
import shutil
import json
from typing import List, Tuple
import RAM as RAM
# import easyocr
import re
# import google.generativeai as genai
from PIL import Image
import time
# New imports for concurrency
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import partial
import threading
from transformers import SiglipProcessor, SiglipModel
from gemini_mistral_server import ApiKeyManager
from Generator import Generator


# (All class definitions like TransNetV2 and other helper methods remain the same)
# ... [TransNetV2 class and other methods from previous answer go here] ...
# NOTE: To keep the answer focused, I am omitting the unchanged code.
# Assume the TransNetV2 class and helper methods from the previous answer are present.
OCR_BATCH_SIZE = 8
MISTRAL_SHEET_URL = os.getenv("MISTRAL_SHEET_URL", "https://docs.google.com/spreadsheets/d/1NAlj7OiD9apH3U47RLJK0en1wLSW78X5zqmf6NmVUA4/export?format=csv&gid=0")
GEMINI_SHEET_URL = os.getenv("GEMINI_SHEET_URL", "https://docs.google.com/spreadsheets/d/1gqlLToS3OXPA-CvfgXRnZ1A6n32eXMTkXz4ghqZxe2I/export?format=csv&gid=0")

API_KEY = ApiKeyManager(
    sheet_url=GEMINI_SHEET_URL
).get_active_key_count()
#prompt
OCR_PROMPT = '''
You are an OCR agent specialized in extracting text from **news-related images** in both **Vietnamese** and **English**.

Your task is to:

- Accurately detect and extract **all visible text** from each input image, including:
  - üì∞ **Main headline / title**
  - üìù **Subheadings / captions**
  - üìÑ **Body text or any other readable content**
- Preserve the **natural reading order** (top to bottom, left to right).
- Recognize both **Vietnamese (without diacritics)** and **English** text.
- **Do not translate** the text‚Äîjust extract it **verbatim** as it appears.
- If an image contains **no readable text**, return an empty string for that frame.
- Input is a **batch of images**, typically extracted from a video (e.g., keyframes).
- Output the extracted texts in a **structured format**, preserving the input order.
- Extract the channel name and time if available, but focus primarily on the main text content.
---

### üì§ Input
A list of image frames, ordered as they appear in the video:
```python
["frame_0001.jpg", "frame_0002.jpg", ..., "frame_N.jpg"]


**Return answer in this json format, no additional word:**
{
    "extracted_texts": [
        "Text from frame_0001",
        "Text from frame_0002",
        ...
        "Text from frame_N"
    ]
}

'''

class TransNetV2:
    """
    TransNetV2 model for shot boundary detection.
    This class detects scene transitions in videos.
    """
    def __init__(self, model_dir=None):
        model_dir = "transnetv2-weights"
        if model_dir is None:
            model_dir = os.path.join(os.path.dirname(__file__), "transnetv2-weights/")
            if not os.path.isdir(model_dir):
                raise FileNotFoundError(f"[TransNetV2] ERROR: {model_dir} is not a directory.")
            else:
                print(f"[TransNetV2] Using weights from {model_dir}.")
        # Configure TensorFlow to limit GPU memory usage
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            try:
                # Limit TensorFlow to use only 30% of GPU memory
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                print("[TransNetV2] TensorFlow GPU memory growth enabled")
            except RuntimeError as e:
                print(f"[TransNetV2] Error setting GPU memory growth: {e}")
                
            # --- OPTION 2: SET A HARD VRAM LIMIT (UNCOMMENT TO USE) ---
            # Use this if you need to enforce a strict memory budget for TransNetV2.
            # For example, limit it to 2048 MB (2 GB).
            # memory_limit_mb = 3072
            # for gpu in gpus:
            #     tf.config.experimental.set_virtual_device_configuration(
            #         gpu,
            #         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit_mb)]
            #     )
            # print(f"[TransNetV2] TensorFlow VRAM usage limited to {memory_limit_mb} MB.")

        self._input_size = (27, 48, 3)
        try:
            self._model = tf.saved_model.load(model_dir)
        except OSError as exc:
            raise IOError(f"[TransNetV2] It seems that files in {model_dir} are corrupted or missing. "
                         f"Re-download them manually and retry. For more info, see: "
                         f"https://github.com/soCzech/TransNetV2/issues/1#issuecomment-647357796") from exc

    def predict_raw(self, frames: np.ndarray):
        assert len(frames.shape) == 5 and frames.shape[2:] == self._input_size, \
            "[TransNetV2] Input shape must be [batch, frames, height, width, 3]."
        frames = tf.cast(frames, tf.float32)

        logits, dict_ = self._model(frames)
        single_frame_pred = tf.sigmoid(logits)
        all_frames_pred = tf.sigmoid(dict_["many_hot"])

        return single_frame_pred, all_frames_pred

    def predict_frames(self, frames: np.ndarray):
        assert len(frames.shape) == 4 and frames.shape[1:] == self._input_size, \
            "[TransNetV2] Input shape must be [frames, height, width, 3]."

        def input_iterator():
            no_padded_frames_start = 25
            no_padded_frames_end = 25 + 50 - (len(frames) % 50 if len(frames) % 50 != 0 else 50)

            start_frame = np.expand_dims(frames[0], 0)
            end_frame = np.expand_dims(frames[-1], 0)
            padded_inputs = np.concatenate(
                [start_frame] * no_padded_frames_start + [frames] + [end_frame] * no_padded_frames_end, 0
            )

            ptr = 0
            while ptr + 100 <= len(padded_inputs):
                out = padded_inputs[ptr:ptr + 100]
                ptr += 50
                yield out[np.newaxis]

        predictions = []
        for inp in input_iterator():
            single_frame_pred, all_frames_pred = self.predict_raw(inp)
            predictions.append((single_frame_pred.numpy()[0, 25:75, 0],
                              all_frames_pred.numpy()[0, 25:75, 0]))

            print("\r[TransNetV2] Processing video frames {}/{}".format(
                min(len(predictions) * 50, len(frames)), len(frames)
            ), end="")
        print("")

        single_frame_pred = np.concatenate([single_ for single_, all_ in predictions])
        all_frames_pred = np.concatenate([all_ for single_, all_ in predictions])

        return single_frame_pred[:len(frames)], all_frames_pred[:len(frames)]

    def predict_video(self, video_fn: str):
        try:
            import ffmpeg
        except ModuleNotFoundError:
            raise ModuleNotFoundError("For `predict_video` function `ffmpeg` needs to be installed...")

        print("[TransNetV2] Extracting frames from {}".format(video_fn))
        video_stream, err = ffmpeg.input(video_fn).output(
            "pipe:", format="rawvideo", pix_fmt="rgb24", s="48x27"
        ).run(capture_stdout=True, capture_stderr=True)

        video = np.frombuffer(video_stream, np.uint8).reshape([-1, 27, 48, 3])
        return (video, *self.predict_frames(video))

    @staticmethod
    def predictions_to_scenes(predictions: np.ndarray, threshold: float = 0.3):
        predictions = (predictions > threshold).astype(np.uint8)
        scenes = []
        t, t_prev, start = -1, 0, 0
        for i, t in enumerate(predictions):
            if t_prev == 1 and t == 0:
                start = i
            if t_prev == 0 and t == 1 and i != 0:
                scenes.append([start, i])
            t_prev = t
        if t == 0:
            scenes.append([start, i])

        if len(scenes) == 0:
            return np.array([[0, len(predictions) - 1]], dtype=np.int32)
        return np.array(scenes, dtype=np.int32)


class VideoKeyframeExtractor:
    def __init__(self, transnet_weights=None, output_dir="keyframes", 
                 sample_rate=1, max_frames_per_shot=16, model_path="google/siglip2-base-patch16-512"):
        self.device = "cuda"
        self.transnet = TransNetV2(transnet_weights)
        self.model, self.preprocess = clip.load('ViT-B/32', self.device)
        self.ram = RAM.load_tag_model()
        self.output_dir = output_dir
        self.sample_rate = sample_rate
        self.max_frames_per_shot = max_frames_per_shot
        print(f"[CLIP] Using device: {self.device}")
        os.makedirs(output_dir, exist_ok=True)
        # Thread-safe counter for frame filenames
        self._frame_counter = 1
        self._counter_lock = threading.Lock()
        # TH√äM KH√ìA GPU N√ÄY:
        self._gpu_lock = threading.Lock()
    def get_video_fps(self, video_path: str) -> float:
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        cap.release()
        return fps

    def extract_video_frames(self, video_path: str) -> Tuple[List[np.ndarray], List[int]]:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Could not open video file: {video_path}")
        frames, frame_indices, frame_idx = [], [], 0
        while True:
            ret, frame = cap.read()
            if not ret: break
            if frame_idx % self.sample_rate == 0:
                frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                frame_indices.append(frame_idx)
            frame_idx += 1
        cap.release()
        return frames, frame_indices
        
    def _get_next_frame_count(self):
        with self._counter_lock:
            count = self._frame_counter
            self._frame_counter += 1
            return count

    def extract_clip_features(self, frames: List[np.ndarray], shot_id: int = None) -> np.ndarray:
        features = []
        batch_size = 8
        progress_prefix = f"[CLIP][Shot {shot_id}]" if shot_id is not None else "[CLIP]"

        # ƒê·∫£m b·∫£o ch·ªâ M·ªòT lu·ªìng truy c·∫≠p GPU t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
        with self._gpu_lock: # <-- TH√äM D√íNG N√ÄY
            for i in range(0, len(frames), batch_size):
                batch_frames = frames[i:i+batch_size]

                # Convert frames to PIL Images and preprocess for CLIP
                batch_inputs = torch.stack([
                    self.preprocess(Image.fromarray(frame))
                    for frame in batch_frames
                ]).to(self.device)

                # Extract features
                with torch.no_grad():
                    batch_features = self.model.encode_image(batch_inputs)
                    batch_features /= batch_features.norm(dim=-1, keepdim=True)  # Normalize

                features.append(batch_features.cpu().numpy())
                print(f"\r[CLIP] Processing frames {i+len(batch_frames)}/{len(frames)}", end="")
                
                # Gi·∫£i ph√≥ng b·ªô nh·ªõ GPU sau m·ªói batch n·∫øu c√≥ th·ªÉ
                del batch_features, batch_inputs
                torch.cuda.empty_cache() 
                # print(f"\r{progress_prefix} Processing frames {i + len(batch_frames)}/{len(frames)}", end="")
            
        print(f"\r{progress_prefix} Feature extraction complete.                ")
        return np.vstack(features)

    def adaptive_clustering(self, features: np.ndarray) -> List[int]:
        n_samples = features.shape[0]
        if n_samples <= 1: return [0] if n_samples == 1 else []
        if n_samples <= 3: return list(range(n_samples))

        k_max = min(int(np.sqrt(n_samples)), 10)
        best_score, best_k, best_labels = -1, 2, None

        for k in range(2, k_max + 1):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
            labels = kmeans.fit_predict(features)
            if len(np.unique(labels)) > 1:
                score = silhouette_score(features, labels)
                if score > best_score:
                    best_score, best_k, best_labels = score, k, labels
                    best_centers = kmeans.cluster_centers_

        if best_labels is None:
            kmeans = KMeans(n_clusters=2, random_state=42, n_init='auto')
            best_labels = kmeans.fit_predict(features)
            best_centers = kmeans.cluster_centers_

        keyframe_indices = []
        for i in range(len(best_centers)):
            cluster_frames = np.where(best_labels == i)[0]
            if len(cluster_frames) > 0:
                dists = np.linalg.norm(features[cluster_frames] - best_centers[i], axis=1)
                keyframe_indices.append(cluster_frames[np.argmin(dists)])

        keyframe_features = features[keyframe_indices]
        n_keyframes = len(keyframe_indices)
        if n_keyframes <= 1: return keyframe_indices

        similarity_matrix = np.dot(keyframe_features, keyframe_features.T)
        to_keep = list(range(n_keyframes))
        threshold = 0.92
        i = 0
        while i < len(to_keep):
            j = i + 1
            while j < len(to_keep):
                if similarity_matrix[to_keep[i], to_keep[j]] > threshold:
                    to_keep.pop(j)
                else: j += 1
            i += 1
        return [keyframe_indices[i] for i in to_keep]

# <<< PASTE THIS CORRECTED FUNCTION INTO YOUR SCRIPT >>>

    def _post_process_batch_concurrently(self, batch_data, tag_model, ocr_model):
        """
        X·ª≠ l√Ω ƒë·ªìng th·ªùi m·ªôt l√¥ (batch) c√°c keyframe.
        - OCR ƒë∆∞·ª£c th·ª±c hi·ªán trong m·ªôt l·ªánh g·ªçi API duy nh·∫•t cho c·∫£ l√¥.
        - Tagging ƒë∆∞·ª£c th·ª±c hi·ªán tu·∫ßn t·ª± cho t·ª´ng ·∫£nh trong l√¥.
        """
        if not batch_data:
            return []

        keyframe_paths, images = zip(*batch_data)
        batch_size = len(keyframe_paths)
        
        tags_results = [[] for _ in range(batch_size)]
        # Initialize with a default value
        ocr_results = ["" for _ in range(batch_size)] 
        
        # 1. Perform batched OCR
        if ocr_model and images:
            try:
                print(f"[PostProc] Submitting batch of {batch_size} frames for OCR...")
                # Convert tuple 'images' to a list for the API call
                response_str, _ = ocr_model.generate(
                    model_name="gemini-2.0-flash", # Using a reliable model
                    images=list(images), 
                    prompt=OCR_PROMPT
                )
                
                # Robustly parse the JSON response
                try:
                    # Clean the string from markdown code fences
                    clean_response_str = re.sub(r'```json\s*|\s*```', '', response_str).strip()
                    response_json = json.loads(clean_response_str)
                    extracted_texts = response_json.get("extracted_texts", [])

                    if isinstance(extracted_texts, list):
                        # Handle cases where the API returns fewer texts than images sent
                        num_returned = len(extracted_texts)
                        for i in range(batch_size):
                            if i < num_returned:
                                ocr_results[i] = extracted_texts[i].strip() if extracted_texts[i] else ""
                            else:
                                # If API returned fewer results, fill the rest with empty strings
                                ocr_results[i] = ""
                        if num_returned != batch_size:
                            print(f"[WARNING] OCR returned {num_returned} results for a batch of {batch_size}. Padding with empty strings.")
                    else:
                        print(f"[WARNING] OCR batch result 'extracted_texts' was not a list. Using empty strings. Response: {response_str}")
                
                except json.JSONDecodeError:
                    print(f"[WARNING] Failed to decode JSON from OCR response. Using empty strings. Response was: {response_str}")

            except Exception as e:
                print(f"[WARNING] OCR batch processing failed entirely: {e}. All OCR results for this batch will be empty.")
                # ocr_results is already initialized to empty strings, so no action needed

        # 2. Perform Tagging for each image in the batch
        if tag_model:
            for i, keyframe_path in enumerate(keyframe_paths):
                try:
                    # The get_tag function likely needs the path, not the image object
                    tags = RAM.get_tag(keyframe_path, tag_model)
                    tags_results[i] = tags
                except Exception as e:
                    print(f"[WARNING] Tagging failed for {os.path.basename(keyframe_path)}: {e}")

        # 3. Combine results and return
        final_results = []
        for i in range(batch_size):
            final_results.append((keyframe_paths[i], tags_results[i], ocr_results[i]))
            
        return final_results

    def _process_shot_concurrently(self, shot_task_data: Tuple) -> Tuple:
        shot_idx, (start_idx, end_idx), all_frames, all_frame_indices = shot_task_data
        shot_frame_indices = [
            i for i, frame_idx in enumerate(all_frame_indices)
            if start_idx <= frame_idx <= end_idx
        ]
        if len(shot_frame_indices) > self.max_frames_per_shot:
            step = len(shot_frame_indices) // self.max_frames_per_shot
            shot_frame_indices = shot_frame_indices[::step]
        if not shot_frame_indices:
            return shot_idx, [], []
        shot_frames = [all_frames[i] for i in shot_frame_indices]
        shot_features = self.extract_clip_features(shot_frames, shot_id=shot_idx + 1)
        keyframe_indices_in_shot = self.adaptive_clustering(shot_features)
        return shot_idx, keyframe_indices_in_shot, shot_frame_indices
    

    # <<< MAIN METHOD REFACTORED FOR STREAMING PIPELINE >>>
    def extract_keyframes(self, video_path: str) -> None:
        # ... (c√°c ph·∫ßn code kh√°c kh√¥ng ƒë·ªïi) ...

        # Reset frame counter   for each new video
        self._frame_counter = 1

        video_name = os.path.splitext(os.path.basename(video_path))[0]
        video_output_dir = os.path.join(self.output_dir, video_name)

        if os.path.exists(video_output_dir): shutil.rmtree(video_output_dir)
        os.makedirs(video_output_dir, exist_ok=True)

        # new_video_path = os.path.join(video_output_dir, os.path.basename(video_path))
        # shutil.copy(video_path, new_video_path)
        # video_path = new_video_path
        print(f"\n[PIPELINE] Starting processing for: {video_path}")

        # --- STAGE 1 (Sequential): Shot Detection and Frame Extraction ---
        fps = self.get_video_fps(video_path)
        _, single_frame_predictions, _ = self.transnet.predict_video(video_path)
        scenes = self.transnet.predictions_to_scenes(single_frame_predictions)
        print(f"[PIPELINE] Detected {len(scenes)} shots.")
        print("[PIPELINE] Extracting all frames...")
        all_frames, all_frame_indices = self.extract_video_frames(video_path)
        print(f"[PIPELINE] Extracted {len(all_frames)} frames.")

        # LOAD RAM MODEL HERE, BEFORE ANY THREADS MIGHT NEED IT FOR POST-PROCESSING
        # (ƒê√¢y l√† m·ªôt s·ª≠a l·ªói logic trong code ban ƒë·∫ßu c·ªßa b·∫°n,
        # v√¨ tag_model c·∫ßn ƒë∆∞·ª£c t·∫£i tr∆∞·ªõc khi c√°c task post-processing ƒë∆∞·ª£c submit)
        # <<< THAY ƒê·ªîI >>>: T·∫£i c√°c model (gi·ªëng nh∆∞ tr∆∞·ªõc)
    # <<< C·∫¢I THI·ªÜN >>>: S·ª≠ d·ª•ng model ƒë√£ ƒë∆∞·ª£c t·∫£i trong __init__ m·ªôt c√°ch nh·∫•t qu√°n
        tag_model = self.ram
        ocr_model = None
        try:
            # tag_model ƒë√£ ƒë∆∞·ª£c t·∫£i
            print("[PIPELINE] RAM tag model is already loaded.")
        except Exception as e:
            print(f"[ERROR] RAM tag model check failed: {e}. Post-processing will skip tagging.")
            tag_model = None # ƒê·∫£m b·∫£o model l√† None n·∫øu c√≥ l·ªói
        try:
            ocr_model = Generator(base_url="http://localhost:9600", api_key=API_KEY)
            print("[PIPELINE] OCR model initialized successfully.")
        except Exception as e:
            print(f"[ERROR] Could not initialize OCR model: {e}. Post-processing will skip OCR.")
        
        all_keyframes_info = []
        metadata_stubs = {}
        metadata = {video_name: {}}
        num_cpu_workers = min(4, os.cpu_count() or 1) 
        print(f"[PIPELINE] Using {num_cpu_workers} workers for shot processing (CPU/GPU overlap).")

        with ThreadPoolExecutor(max_workers=num_cpu_workers, thread_name_prefix='ShotProc') as shot_executor, \
             ThreadPoolExecutor(max_workers=6, thread_name_prefix='PostProc') as post_proc_executor:

            shot_futures = {
                shot_executor.submit(
                    self._process_shot_concurrently,
                    (shot_idx, scene, all_frames, all_frame_indices)
                ): shot_idx
                for shot_idx, scene in enumerate(scenes)
            }
            post_processing_futures = []
            keyframe_batch_buffer = []

            print(f"\n[PIPELINE] Submitted {len(shot_futures)} shots for sequential processing. Starting streaming pipeline...")

            for future in as_completed(shot_futures):
                shot_idx = shot_futures[future]
                try:
                    s_idx, keyframe_indices_in_shot, shot_frame_indices = future.result()
                    if not keyframe_indices_in_shot:
                        continue
                    
                    print(f"[PIPELINE] Shot {s_idx+1} completed. Found {len(keyframe_indices_in_shot)} keyframes. Buffering for post-processing...")

                    for i, keyframe_idx_in_shot in enumerate(keyframe_indices_in_shot):
                        frame_count = self._get_next_frame_count()
                        original_sampled_idx = shot_frame_indices[keyframe_idx_in_shot]
                        original_frame_idx = all_frame_indices[original_sampled_idx]
                        
                        frame_filename = f"frame_{frame_count:03d}.webp"
                        keyframe_path = os.path.join(video_output_dir, frame_filename)

                        image = Image.fromarray(all_frames[original_sampled_idx])
                        image.save(keyframe_path, 'WEBP', quality=90)
                        
                        keyframe_batch_buffer.append((keyframe_path, image))
                        
                        if len(keyframe_batch_buffer) >= OCR_BATCH_SIZE:
                            print(f"[PIPELINE] Submitting a batch of {len(keyframe_batch_buffer)} keyframes for post-processing.")
                            task_future = post_proc_executor.submit(
                                self._post_process_batch_concurrently,
                                keyframe_batch_buffer[:],
                                tag_model,
                                ocr_model
                            )
                            post_processing_futures.append(task_future)
                            keyframe_batch_buffer.clear()
                        
                        timestamp = f"{(original_frame_idx / fps) // 60:02.0f}:{(original_frame_idx / fps) % 60:06.3f}"
                        metadata_stubs[keyframe_path] = {
                            "frame_name": f"frame_{frame_count:03d}",
                            "id": original_frame_idx,
                            "time-stamp": timestamp,
                            "shot": s_idx + 1,
                        }
                        all_keyframes_info.append((keyframe_path, s_idx, i, original_frame_idx))
                except Exception as exc:
                    print(f'[PIPELINE] Shot {shot_idx + 1} generated an exception: {exc}')

            if keyframe_batch_buffer:
                print(f"[PIPELINE] Submitting the final batch of {len(keyframe_batch_buffer)} keyframes.")
                task_future = post_proc_executor.submit(
                    self._post_process_batch_concurrently,
                    keyframe_batch_buffer[:],
                    tag_model,
                    ocr_model
                )
                post_processing_futures.append(task_future)
                keyframe_batch_buffer.clear()

            # <<< S·ª¨A L·ªñI >>>: X√ìA V√íNG L·∫∂P TH·ª® HAI. Ch·ªâ gi·ªØ l·∫°i v√≤ng l·∫∑p ƒê√öNG n√†y.
            print(f"\n[PIPELINE] All shots processed. Aggregating results from {len(post_processing_futures)} batches...")
            for future in as_completed(post_processing_futures):
                try:
                    batch_results = future.result()
                    if not batch_results: continue # B·ªè qua n·∫øu l√¥ kh√¥ng c√≥ k·∫øt qu·∫£

                    for keyframe_path, tags, ocr_text in batch_results:
                        if keyframe_path in metadata_stubs:
                            stub = metadata_stubs[keyframe_path]
                            frame_name = stub.pop("frame_name")
                            metadata[video_name][frame_name] = {
                                **stub, 
                                "tags": tags,
                                "ocr": ocr_text
                            }
                except Exception as exc:
                    print(f'[PIPELINE] A post-processing batch generated an exception: {exc}')
            
        # --- STAGE 4 (Finalization): Save all files ---
        # Sort metadata by frame number for consistent output
        sorted_frames = sorted(metadata[video_name].items(), key=lambda item: int(item[0].split('_')[1]))
        metadata[video_name] = dict(sorted_frames)

        metadata_path = os.path.join(video_output_dir, "metadata.json")
        with open(metadata_path, "w", encoding='utf-8') as f:
            json.dump(metadata, f, indent=3, ensure_ascii=False)
        print(f"[PIPELINE] Metadata saved to {metadata_path}")
        
        # Sort summary info for readability
        all_keyframes_info.sort(key=lambda x: int(os.path.basename(x[0]).split('_')[1].split('.')[0]))
        with open(os.path.join(video_output_dir, "keyframes_summary.txt"), "w") as f:
            f.write(f"Video: {os.path.basename(video_path)}\nTotal shots: {len(scenes)}\nTotal keyframes: {len(all_keyframes_info)}\n\n")
            for path, s_idx, k_idx, o_idx in all_keyframes_info:
                f.write(f"{os.path.basename(path)}: Shot {s_idx+1}, Keyframe {k_idx+1}, Original Frame {o_idx}\n")


def main():
    parser = argparse.ArgumentParser(description="Extract keyframes from videos using a streaming pipeline.")
    parser.add_argument("video_path", type=str, help="Path to the video file or directory of videos")
    parser.add_argument("--output", type=str, default="keyframes", help="Output directory for keyframes")
    parser.add_argument("--sample-rate", type=int, default=5, help="Sample every N frames to reduce computation")
    parser.add_argument("--max-frames", type=int, default=55, help="Maximum number of frames to process per shot")
    args = parser.parse_args()

    start = time.time()
    extractor = VideoKeyframeExtractor(
        transnet_weights="transnetv2-weights",
        output_dir=args.output,
        sample_rate=args.sample_rate,
        max_frames_per_shot=args.max_frames
    )
    
    extractor.extract_keyframes(args.video_path)
    end = time.time()
    last = end - start
    print(f"Total time taken for keyframe extraction: {last:.2f} seconds")

if __name__ == "__main__":
    main()


    # python infer_concurent.py /workspace/WorkingSpace/Personal/chinhnm/data/L01_V002.mp4 --output /workspace/WorkingSpace/Personal/chinhnm/dmdm_demo